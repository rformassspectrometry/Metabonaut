---
title: "Normalization and feature selection with the notame"
format: html
tbl-cap-location: bottom
editor: visual
minimal: true
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
vignette: >
  %\VignetteIndexEntry{Complete end-to-end LC-MS/MS Metabolomic Data analysis}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
library(knitr)
library(quarto)
knitr::opts_knit$set(root.dir = './')
```

# Introduction
Herein, we will perform data pretreatment and feature selection using the *notame* R package, which was developed in parallel with the associated protocol article published in the "Metabolomics Data Processing and Data Analysiss—Current Best Practices” special issue of the Metabolites journal (Klåvus et al. 2020) [@klavus_notame_2020]. The main outcome is identifying interesting features for laborious downstream steps relating to biological context, such as annotation and pathway analysis, which fall outside the purview of *notame*. The associated protocol article presents a sequence of steps that is easily adopted by new practitioners. We will not follow the protocol article exactly and instead do an analysis which meshes with the trunk of the workflow.


# Setup
Let's attach the *notame* and load the preprocessed data as a SummarizedExperiment object returned from MSExperiment::quantify().

```{r, message=FALSE, warning=FALSE}
library(doParallel)
library(cowplot)
library(limma)
library(notame)
library(SummarizedExperiment)

registerDoParallel(cores = 14)

res <- readRDS("res.RDS")
```

The SummarizedExperiment container used for the data is very similar to the 
MetaboSet container used in *notame*. The MetaboSet container is derived from the widely used ExpressionSet container, and is extended with three slots which can store the names of the columns for study group, subject identifier and time point. The accessors are a bit different from SummarizedExperiment; use exprs() to access the abundances, fData() to access feature metadata and pData() to access sample metadata. The sample and feature metadata needs to be modified somewhat for the MetaboSet container. 

```{r, message=FALSE, warning=FALSE}
pheno_data <- colData(res)
feature_data <- rowData(res)

# Rename columns of sample metadata
rename_ind <- which(names(pheno_data) %in% 
                    c("sample_name", "sample_type", "injection_index"))
names(pheno_data)[rename_ind] <- c("Sample_ID", "QC", "Injection_order")

# Match the rownames of sample metadata with Sample_ID
rownames(pheno_data) <- pheno_data$Sample_ID

# Change name of pooled samples to "QC" in the "QC" column
pheno_data$QC[pheno_data$QC == "pool"] <- "QC"

# Convert phenotype column to factor
pheno_data$phenotype %<>% as.factor()

# Create Feature_ID column in feature metadata
feature_data <- data.frame(Feature_ID = rownames(feature_data), feature_data)

# Create "Split" column in feature data with analytical mode
feature_data$Split <- "HILIC_pos"

# Construct MetaboSet
res <- construct_metabosets(exprs = assay(res), 
                            pheno_data = as.data.frame(pheno_data), 
                            feature_data = as.data.frame(feature_data),
                            group_col = "phenotype", split_data = FALSE)
```

# Normalization
Features with a low detection rate are flagged, as they are deemed too unreliable not only for statistical analysis, but also for the normalization process which relies heavily on QC samples. Flagged features are automatically excluded in *notame*, but can be included using `all_features = TRUE` in many functions. We set the detection rate threshold for QC samples at 70% as per Broadhurst et al. 2018 [@qcguidelines], plus a within-group threshold of 80%. The within-group threshold must be met in at least one study group.

```{r, message=FALSE, warning=FALSE}
detected <- flag_detection(res, qc_limit = 0.7, group_limit = 0.8)
```

`r sum(!is.na(flag(detected)))` features were flagged for detection rate, so now we  have reduced the number of features from `r sum(nrow(detected))` to `r nrow(drop_flagged(detected))`. Brief notes about the flagging was stored in feature metadata.

Next, we correct for drift in features using a cubic spline, relating each features' abundance in QC samples to injection order. Drift correction is performed in log-space since the log transformed data better follows assumptions of cubic spline regression. The value for the smoothing parameter is, by default, optimized using leave-one-out cross-validation within a range of 0.5 to 1.5 for balance between overfitting and reducing unwanted variation. The abundances are corrected by adding the mean of a feature’s abundance in the QC samples and subtracting the predicted fit for each feature.

```{r, message=FALSE, warning=FALSE, results="hold"}
drift_normalized <- correct_drift(drop_flagged(detected))
```

`r sum(fData(drift_normalized)$DC_note == "Drift_corrected")` features were corrected for drift. `r sum(fData(drift_normalized)$DC_note == "Missing_QCS")` features had missing QC values after flagging for detection rate. These were not corrected because at least four QC samples with values are needed for fitting the cubic spline.

Next, we apply probabilistic quotient normalization (PQN) to reduce unwanted variation from differential dilution of samples. The central challenge here is to reduce unwanted variation from dilution, whether biological or experimental, while accounting for the possibility of biologically meaningful variation in total feature abundances. In probabilistic quotient normalization, the most probable dilution factor is determined for each sample as the median of quotients calculated for each feature relative to the median of each feature's abundance in reference samples. The sample abundances are then divided by the dilution factor. Below, we use QC samples as reference samples as the drift correction methodology in *notame* also assumes that the QC samples are representative of the biological samples.

```{r, message=FALSE, warning=FALSE}
dilution_normalized <- pqn_normalization(drift_normalized, ref = "qc", 
                                         method = "median")
```

Next, let's visualize the data before and after normalization for drift and dilution. Typically, the data would be assessed visually after each step of the normalization process, using the visualizations() wrapper to save relevant plots. Herein, the data is visualized side-by-side after normalization for both drift and dilution for conciseness. 

It seems that the small number of samples does not allow us to fit linear models relating abundance to injection order properly for all samples, as evidenced by the overabundance of high p-values in the top histograms. This is likely because the QC samples and biological samples are so dissimilar. Considering only the biological samples, the distribution is more uniform, especially after normalization. Typically, drift in the biological samples would be evidenced by an overabundance of low p-values. In this case, the small sample size and drift adding to the variation in biological samples could explain the deviation from the uniform distribution. Insofar as this is the case, the more uniform distribution after normalization indicates less drift in the biological samples. In the case of QC samples, the uniform distribution is evidence for drift as the p-value is expected to be one for all features given that the samples are identical.

```{r, results = 'hide'}
#| fig-cap: "Figure 1. P-values from linear regression models relating each feature to injection order. The dashed red lines represent the expected uniform distribution. A) Before normalization, featuring all samples, biological samples and QC samples. B) After normalization, featuring all samples, biological samples and QC samples"
#| fig-width: 16
#| fig-height: 7
#| out-width: "100%"
plot_grid(plot_injection_lm(detected),
          plot_injection_lm(dilution_normalized),
          labels = "AUTO")
```

The distributions of feature abundances are somewhat shifted after normalization. The QC samples appear not to have been affected by PQN, although the  differences were very small to begin with. There also doesn't seem to be a global trend with regard to injection order before or after normalization. Depending on the instrumentation and experimental details, one may see a trend or the drift cancel out, as seems to be the case here. The distributions of feature abundances are a bit more similar between study groups after normalization, suggesting that there could be a difference in the biological dilution of the samples between the study groups. Within study groups, the differences are accentuated a little, which may point to experimental or biological dilution-related variation within study groups. Finally, PQN decreased generally decreased the abundances of the biological samples to levels more similar to the QC samples. This comes from a dilution factor of over one, reflecting the different source and processing of QC samples. This may limit the utility of PQN normalization in preserving biological variation of interest.

```{r, warning=FALSE}
#| fig-cap: "Figure 2. Boxplots representing the distributions of feature intensities in each sample by injection order, featuring the median as a black line, the interquartile range as a box and the 1.5x the interquartile range as whiskers. A) Before normalization. B) After normalization."
#| fig-width: 15
#| fig-height: 7
#| out-width: "100%"
plot_grid(plot_sample_boxplots(detected, 
                               order_by = "Injection_order"), 
          plot_sample_boxplots(dilution_normalized, 
                               order_by = "Injection_order"),
          labels = "AUTO")
```

For the remaining visualizations which all involve Euclidean distances, the data was autoscaled. After normalization, the density plot shows a single density peak for QC samples attesting to reduction of unwanted variation. On the other hand, there seems to be increased distance between some QC samples, as evidenced by the flattened density peak. The origin of this is unclear, but may point to problems in fitting the cubic spline to only four QC samples. Interpreting the distance between the biological samples is a bit challenging due to the scaling. There is perhaps more clearly a single peak, indicating increased similarity between biological samples. The separation between qc samples and biological samples has not increased, although in this case it may be an artifact from the limitations affecting the kernel density estimate.

```{r}
#| fig-cap: "Figure 3. Density plot of Euclidean distances between samples. A) Before normalization. B) After normalization."
#| fig-width: 16
#| fig-height: 7
#| out-width: "100%"
plot_grid(plot_dist_density(detected, title = NULL), 
          plot_dist_density(dilution_normalized, title = NULL),
          labels = "AUTO")
```

PCA is useful for visualizing patterns in the data. The overall structure is intact, indicating that the biological variation of interest has not been attenuated. Moreover, the QC samples group more tightly, indicating that we have reduced unwanted variation. The CVD group seems to cluster together more tightly after normalization, although the opposite can be said for the CTR group. Such local differences could be investigated further using t-SNE and coloring for injection order, hoping for tighter groups and dissipated trends. There is little difference in the variance explained by the first two components.

```{r}
#| fig-cap: "Figure 4. PCA plots of samples, shape by study group and color by injection order. A) Before normalization. B) After normalization."
#| fig-width: 10.5
#| fig-height: 4
#| out-width: "100%"
plot_grid(plot_pca(detected, color = "Injection_order", 
                   shape = "phenotype"),
          plot_pca(dilution_normalized, color = "Injection_order", 
                   shape = "phenotype"), 
          labels = "AUTO")
```

PCA and hierarchical clustering using Ward's criterion are complementary unsupervised methods operating in Euclidean space. While PCA focuses on explaining the variation with maximally reduced dimensionality, hierarchical clustering allows observation of relationships between samples at high resolution. It is interesting to notice that after normalization, one CTR sample clusters more closely with the CVD samples, as in the PCA both before and after normalization. One interpretation of this is that the discarded information in PCA, providing little information regarding the separation of study groups, drives the clustering. This could be a sign of that the separation between study groups is weak and unstable.

```{r}
#| fig-cap: "Figure 5. Dendrograms of hierarchical sample clusters using Ward’s criterion on Euclidean distances between samples. A) Before normalization. B) After normalization."
#| fig-width: 20
#| fig-height: 10
#| out-width: "100%"
plot_grid(plot_dendrogram(detected, title = NULL, subtitle = ""),
          plot_dendrogram(dilution_normalized, title = NULL, subtitle = ""),
          labels = "AUTO")
```

As expected, the distances between QC samples are smaller after normalization. Broadly speaking, the distances between biological samples seem to have decreased, although distances of biological samples relative to QC samples have increased as evidenced by the slightly lighter overall appearance. As we noted earlier, there was little evidence for drift, so this relative shift may be attributed to PQN accounting for variation of interest in the biological samples. The shift is most clear for the C sample that clustered apart from its study group, possibly substantiating that dilution explains much of the separation between the study groups. 

```{r}
#| fig-cap: "Figure 6. Heatmaps of Euclidean distances between samples, grouped by hierarchical clusters using Ward's criterion. A) Before normalization. B) After normalization."
#| fig-width: 20
#| fig-height: 10
#| out-width: "100%"
plot_grid(plot_sample_heatmap(detected, title = NULL, subtitle = ""),
          plot_sample_heatmap(dilution_normalized, title = NULL, subtitle = ""),
          labels = "AUTO")
```

We have reduced unwanted variation as evidenced by QC samples, but we may have reduced biological variation of interest as well and and another analyst may interpret the visualizations differently. This would not be surprising considering the source of the QC samples and the small sample size.

# Feature prefiltering
Assuming that we are happy with the normalization process and have a feel for the data, we flag low-quality features, excluding them from downstream steps. 

In addition to the D-ratio and detection rate in the trunk of the workflow, we filter features by their relative standard deviation in QC samples. The non-parametric, robust versions of the D-ratio and relative standard deviation are used as per Broadhurst et al. ().

```{r, message=FALSE, warning=FALSE}
filtered <- flag_quality(dilution_normalized, 
                         condition = "RSD_r < 0.2 & D_ratio_r < 0.4")
```

We're down to 2976 features. Next, we impute missing values. Using random forest imputation, we can accommodate the possibility of missing values arising not only from the limit of detection, but also because of imperfect gap-filling. We drop QC samples so as not to bias the imputation. The strict thresholds for quality metrics and detection rate allow us to be more confident in our imputed values.

```{r, message=FALSE, warning=FALSE, results='hold'}
set.seed(2024)
imputed <- impute_rf(drop_qcs(filtered))
base <- drop_flagged(imputed)
```

The out-of-bag error is promising. Now the data is ready for feature selection.

# Differential abundance analysis
Let's get a final look at the data with PCA, this time without QC samples. 

```{r}
#| fig-cap: "Figure 7. PCA plot of biological samples, shape by study group and color by age."
#| fig-width: 7
#| fig-height: 7
#| out-width: "100%"
plot_pca(imputed, color = "age", shape = "phenotype")
```

Motivated by demonstration purposes and the slight age pattern in the PCA plot, we are interested in also including age as an independent variable. We'll use base R linear models to find interesting features, adjust for false positives from multiple testing using the false discovery rate approach and plot the results in histograms to assess validity of the tests and get a feel for the results.

```{r, message=FALSE, warning=FALSE, results = FALSE}
#| fig-cap: "Figure 8. Linear regression p-value histograms with abundance and age as independent variables. A) p-values. B) FDR-adjusted p-values (q-values)."
#| fig-width: 15
#| fig-height: 7
#| out-width: "100%"
lm_results <- perform_lm(log2(base), formula_char = "Feature ~ phenotype + age")
base <- join_fData(base, lm_results[, c("Feature_ID", "phenotypeCVD_P", 
                                        "phenotypeCVD_P_FDR" )])
                                        
plot_grid(plot_p_histogram(list(lm_results$phenotypeCVD_P)),
          plot_p_histogram(list(lm_results$phenotypeCVD_P_FDR)),
          labels = "AUTO")
```

The p-value histogram for looks promising; it is a relatively uniform distribution with an overabundance of low p-values. However, there are no significant features after correction for multiple testing. The FDR correction results in a single, lowest q-value shared across tens of features because they have very similar p-values. 

Above, we essentially performed the initial model fitting part of limma as in the trunk of the workflow, but without the eBayes adjustment. We'll demonstrate limma below to show how eBayes shifts the results, and maybe get an interesting feature or two to plot as well.

```{r, message=FALSE, warning=FALSE}
#| fig-cap: "Figure 9. Linear regression (limma) p-value histograms with abundance and age as independent variables. A) p-values. B) FDR-adjusted p-values (q-values)."
#| fig-width: 15
#| fig-height: 7
#| out-width: "100%"
age <- base$age
phenotype <- factor(base$phenotype)
design <- model.matrix(~ phenotype + age)

# Fit models and moderate t-statistics
fit <- lmFit(log2(exprs(base)), design = design)
fit <- eBayes(fit)

# Initial model-fitting step of limma is base::lm()
t <- fit$coef/fit$stdev.unscaled/fit$sigma
p.value <- 2 * pt(-abs(t), df = fit$df.residual)[, "phenotypeCVD"]
p.adjusted <- p.adjust(p.value, method = "BH")
all.equal(p.adjusted, fData(base)$phenotypeCVD_P_FDR, check.attributes = FALSE)

# Extract results
res <- data.frame(
    Feature_ID = fData(base)$Feature_ID,
    phenotypeCVD_P_limma = fit$p.value[, "phenotypeCVD"],
    phenotypeCVD_P_FDR_limma = p.adjust(fit$p.value[, "phenotypeCVD"], 
                                        method = "BH"),
    phenotypeCVD_coef_limma = fit$coefficients[, "phenotypeCVD"])
    
base <- join_fData(base, res)

plot_grid(plot_p_histogram(list(fData(base)$phenotypeCVD_P_limma)),
          plot_p_histogram(list(fData(base)$phenotypeCVD_P_FDR_limma)),
          labels = "AUTO")
```

Again the p-value histogram looks reasonable, but there are no significant features. For demonstration purposes, we'll consider the feature with the lowest q-value most interesting.

```{r}
fname <- rownames(base[which.min(fData(base)$phenotypeCVD_P_FDR_limma)])
f_pvalue <- min(fData(base)$phenotypeCVD_P_limma)
f_qvalue <- min(fData(base)$phenotypeCVD_P_FDR_limma)
```

After differential abundance analysis or feature selection using supervised learning, for example, the number of significant features or an  ranking cutoff often allows for manual inspection of feature-wise plots. *notame* includes a variety of feature-wise adaptable to a variety of study designs and plotting functionality for drift correction. Herein, we will visualize the lowest q-value feature `r fname` (q = `r round(f_qvalue, 3)`) feature with a beeswarm plot. The distribution is somewhat different after normalization. 

```{r, results=FALSE}
#| fig-cap: "Figure 10. Beeswarm plots for the lowest p-value feature. A) Before normalization. B) After normalization."
#| fig-width: 14
#| fig-height: 7
#| out-width: "100%"
plot_grid(save_beeswarm_plots(detected[fname, ], add_boxplots = TRUE, 
                              save = FALSE)[[1]],
          save_beeswarm_plots(base[fname, ], 
                              add_boxplots = TRUE, save = FALSE)[[1]], 
          labels = "AUTO")
```

The results are also visualized with a variety of comprehensive visualizations. The volcano plot below shows that feature `r fname` is distinct and has the largest fold change of all features. The abundance of `r fname` is, on average, `r round(2^-fData(base)["FT0845", "phenotypeCVD_coef_limma"])` times higher in the CTR group.

Manhattan plots and cloud plots could also be used to inspect how interesting features relate to m/z and retention time. We often co-visualize results from differential abundance analysis with a ranking of features from supervised learning for a combined perspective.

```{r, message=FALSE, warning=FALSE}
#| fig-cap: "Figure 11. Volcano plot of p-values (negative log10 scale) from limma related to fold-change (log2 scale)."
#| out-width: "100%"
volcano_plot(base, x = "phenotypeCVD_coef_limma",
             p = "phenotypeCVD_P_limma",
             label = "Feature_ID",
             label_limit = f_pvalue + 0.0001)
```

# Conclusion
In the trunk of the workflow, "FT0845" was identified as caffeine. Caffeine is a diuretic and can concentrate blood in high doses, so a difference in biological dilution between the study groups can be expected. As there was little evidence for drift, the different normalization for dilution likely explains the difference in results compared to the trunk of the workflow. Due to the different source of the QC samples, the most probable dilution factors may not have been determined accurately for the biological samples. We also filtered out more low-quality features, largely due to the additional RSD criterion. This is also true of two features found significant features in the trunk of the workflow, "FT0371" and "FT5606".